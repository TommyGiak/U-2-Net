{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdccbdc2",
   "metadata": {},
   "source": [
    "# 0 - Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f666e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as standard_transforms\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from data_loader import Rescale\n",
    "from data_loader import RescaleT\n",
    "from data_loader import RandomCrop\n",
    "from data_loader import ToTensor\n",
    "from data_loader import ToTensorLab\n",
    "from data_loader import SemanticSalObjDataset\n",
    "\n",
    "from model import SU2NET\n",
    "from model import SU2NETP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1349c1",
   "metadata": {},
   "source": [
    "# 1 - Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------- 1. define loss function --------\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "def muti_ce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v):\n",
    "\n",
    "\tloss0 = ce_loss(d0,torch.argmax(labels_v, dim=1))\n",
    "\tloss1 = ce_loss(d1,torch.argmax(labels_v, dim=1))\n",
    "\tloss2 = ce_loss(d2,torch.argmax(labels_v, dim=1))\n",
    "\tloss3 = ce_loss(d3,torch.argmax(labels_v, dim=1))\n",
    "\tloss4 = ce_loss(d4,torch.argmax(labels_v, dim=1))\n",
    "\tloss5 = ce_loss(d5,torch.argmax(labels_v, dim=1))\n",
    "\tloss6 = ce_loss(d6,torch.argmax(labels_v, dim=1))\n",
    "\n",
    "\tloss = loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6\n",
    "\tprint(\"l0: %3f, l1: %3f, l2: %3f, l3: %3f, l4: %3f, l5: %3f, l6: %3f\\n\"%(loss0.data.item(),loss1.data.item(),loss2.data.item(),loss3.data.item(),loss4.data.item(),loss5.data.item(),loss6.data.item()))\n",
    "\n",
    "\treturn loss0, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ec1965",
   "metadata": {},
   "source": [
    "# 2 - Getting the paths to training directories and dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eeae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------- 2. set the directory of training dataset --------\n",
    "\n",
    "print('TRAINING WITH SYNTH IMAGES!')\n",
    "\n",
    "\n",
    "batch_size_train = 4\n",
    "\n",
    "\n",
    "tra_img_name_list = glob.glob('train_data_synthetic/wounds/' + '*.png')\n",
    "\n",
    "tra_lbl_name_list = []\n",
    "for img_path in tra_img_name_list:\n",
    "    img_name = img_path.split('/')[-1]\n",
    "\n",
    "    aaa = img_name.split(\".\")\n",
    "    bbb = aaa[0:-1]\n",
    "    imidx = bbb[0]\n",
    "    for k in range(1,len(bbb)):\n",
    "        imidx = imidx + \".\" + bbb[k]\n",
    "\n",
    "    tra_lbl_name_list.append('train_data_synthetic/semantic_masks/' + imidx + label_ext)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"train images: \", len(tra_img_name_list))\n",
    "print(\"train labels: \", len(tra_lbl_name_list))\n",
    "print(\"---\")\n",
    "\n",
    "train_num = len(tra_img_name_list)\n",
    "\n",
    "image_ext = '.png'\n",
    "label_ext = '.png'\n",
    "\n",
    "\n",
    "salobj_dataset = SemanticSalObjDataset(\n",
    "    img_name_list=tra_img_name_list,\n",
    "    lbl_name_list=tra_lbl_name_list,\n",
    "    transform=transforms.Compose([\n",
    "        RescaleT(320),\n",
    "        RandomCrop(288),\n",
    "        ToTensorLab(flag=0)]))\n",
    "\n",
    "salobj_dataloader = DataLoader(salobj_dataset, batch_size=batch_size_train, shuffle=True, num_workers=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33abddaa",
   "metadata": {},
   "source": [
    "# 3 - Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 100\n",
    "\n",
    "### Training with synthetic images\n",
    "\n",
    "print('TRAINING WITH SYNTH IMAGES!')\n",
    "\n",
    "model_name = 'u2net' #'u2netp'\n",
    "\n",
    "# ------- 3. define model --------\n",
    "# define the net\n",
    "if(model_name=='u2net'):\n",
    "    net = SU2NET(in_ch=3, out_ch=4)\n",
    "elif(model_name=='u2netp'):\n",
    "    net = SU2NETP(in_ch=3, out_ch=4)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "\n",
    "# ------- 4. define optimizer --------\n",
    "print(\"---define optimizer...\")\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0004, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
    "#scheduler = torch.optim.lr_scheduler.LinearLR(optimizer=optimizer, start_factor=1, end_factor=1/10, total_iters=50)\n",
    "\n",
    "# ------- 5. training process --------\n",
    "print(\"---start training...\")\n",
    "ite_num = 0\n",
    "running_loss = 0.0\n",
    "running_tar_loss = 0.0\n",
    "ite_num4val = 0\n",
    "save_frq = 10\n",
    "lossi = []\n",
    "\n",
    "for epoch in range(0, epoch_num):\n",
    "    net.train()\n",
    "\n",
    "    for k, data in enumerate(salobj_dataloader):\n",
    "        ite_num = ite_num + 1\n",
    "        ite_num4val = ite_num4val + 1\n",
    "\n",
    "        inputs, labels = data['image'], data['label']\n",
    "\n",
    "        inputs = inputs.type(torch.FloatTensor)\n",
    "        labels = labels.type(torch.FloatTensor)\n",
    "\n",
    "        # wrap them in Variable\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_v, labels_v = Variable(inputs.cuda(), requires_grad=False), Variable(labels.cuda(),\n",
    "                                                                                        requires_grad=False)\n",
    "        else:\n",
    "            inputs_v, labels_v = Variable(inputs, requires_grad=False), Variable(labels, requires_grad=False)\n",
    "\n",
    "        # y zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        d0, d1, d2, d3, d4, d5, d6 = net(inputs_v)\n",
    "        loss2, loss = muti_ce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "\n",
    "        # # print statistics\n",
    "        running_loss += loss.data.item()\n",
    "        running_tar_loss += loss2.data.item()\n",
    "        lossi.append(loss.item())\n",
    "\n",
    "        # del temporary outputs and loss\n",
    "        del d0, d1, d2, d3, d4, d5, d6, loss2, loss\n",
    "    \n",
    "    \n",
    "        print(\"[train synthetic, epoch: %3d/%3d, batch: %5d/%5d, ite: %d] train loss: %3f, tar: %3f \" % (\n",
    "        epoch + 1, epoch_num, (k + 1) * batch_size_train, train_num, ite_num, running_loss / ite_num4val, running_tar_loss / ite_num4val))\n",
    "    \n",
    "    os.makedirs('saved_models/u2net_synth_full/',exist_ok=True)\n",
    "    if (epoch+1) % save_frq == 0:\n",
    "        torch.save(net.state_dict(), 'saved_models/u2net_synth_full/' + model_name + \"_bce_itr_%d_train_%3f_tar_%3f.pth\" % (epoch+1, running_loss / ite_num4val, running_tar_loss / ite_num4val))\n",
    "        running_loss = 0.0\n",
    "        running_tar_loss = 0.0\n",
    "        net.train()  # resume train\n",
    "        ite_num4val = 0\n",
    "\n",
    "pd.DataFrame(lossi).to_csv('saved_models/u2net_synth_full/'+'lossi.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b22d364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8af3db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a643b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f62cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f217efcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a492771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565deecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
